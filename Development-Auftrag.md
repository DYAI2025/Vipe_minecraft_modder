Ziel & Lieferumfang dieses Dev-Auftrags Ziel: Implementiere ein Settings-System (TypeScript Types + JSON Schema + Validierung + Persistenz) und die IPC Contracts für:  stt.streamStart, stt.streamPush, stt.streamStop, stt.streamCancel, stt.streamStatus  llm.healthCheck  llm.completeJSON  … so, dass Cloud-STT (z.B. LiveKit/ähnlich) und Cloud/Local LLM (OpenAI-kompatibel; z.B. lokales Qwen2.5 via vLLM/Ollama) austauschbar konfigurierbar sind – ohne API-Keys im Renderer.  Scope (dieser Auftrag):  Settings Datei lesen/schreiben + Migration + Schema-Validierung  Key/Secret Handling via secretRef (Renderer sieht keinen Klartext-Key)  IPC Typen + Preload Bridge + Main Handler + Renderer Client  Minimal “HealthCheck UI Hook” (nur die IPC-Nutzung, kein komplettes Settings-UI falls ihr das separat macht)  Non-Scope (in diesem Auftrag nicht):  Vollständige LiveKit/Neuphonic SDK Integration (Provider-Implementierung kann als Stub/Adapter beginnen)  Vollständige Voice-UI / Audio-Capture (nur Contract + Datenpfad definieren; Capture kann danach kommen)  Autopilot Pattern-Engine (nur llm.completeJSON als Low-Level Primitive)  Logisch scheint mir, Repo-Ort & File-Layout, damit es “ready to implement” ist Empfohlen: ein kleines Shared-Package für Contracts + Schemas, damit Renderer/Main/Tests exakt dieselben Typen nutzen.  packages/   ipc-contracts/     src/       index.ts                // TS types (Requests/Responses/Events)       channels.ts             // channel name constants       schemas/         settings.schema.json         llm.completeJSON.schema.json (optional, für runtime validation)         stt.streamStart.schema.json  (optional) apps/   studio-electron/     src/       main/         settingsStore.ts       // load/save/validate/migrate, secret store interface         ipcHandlers/           stt.ts           llm.ts           settings.ts          // optional: get/set endpoints       preload/         bridge.ts              // exposes typed APIs       renderer/         ipc/           client.ts            // typed wrapper for preload APIs Faktisch korrekt sage ich, exakte Settings Config (TypeScript Types) 1) SettingsConfig TypeScript // packages/ipc-contracts/src/index.ts  export type LocaleTag = "de-DE" | "en-US";  export type SecretRef = `secret:${string}`; // Example: "secret:livekit_api_key" or "secret:openai_api_key"  export type SttProviderId = "livekit" | "webspeech" | "manual_text";  export interface SttCommonConfig {   provider: SttProviderId;   language: LocaleTag;          // default: "de-DE"   sampleRateHz: 16000 | 48000;  // must match capture/provider expectations   interimResults: boolean;      // default: true   maxUtteranceMs: number;       // default: 120000   endpointingMs: number;        // default: 800 (silence to finalize) }  export interface SttLiveKitConfig {   provider: "livekit";   /** OpenAI-compatible or provider-specific STT endpoint (HTTP/WSS) your main process talks to. */   endpointUrl: string;          // e.g. https://... or wss://...   apiKeyRef?: SecretRef;        // stored via OS keychain or electron safeStorage wrapper   /** Optional model/engine name */   model?: string;   /** If true, TLS verification is disabled (dev only). */   insecureSkipVerifyTLS?: boolean; }  export interface SttWebSpeechConfig {   provider: "webspeech";   // WebSpeech runs in renderer; still keep config for selection.   // For v0.1 you may implement as renderer-native and bypass stt.stream*. }  export interface SttManualTextConfig {   provider: "manual_text";   // No voice; text input fallback. }  export type SttProviderConfig = SttLiveKitConfig | SttWebSpeechConfig | SttManualTextConfig;  export type LlmProviderId = "openai_compatible";  export type LlmSafetyModeDefault = "actions_only" | "patch_fix";  export interface LlmOpenAICompatibleConfig {   provider: "openai_compatible";   baseUrl: string;              // e.g. https://api.../v1 or http://127.0.0.1:11434/v1   apiKeyRef?: SecretRef;        // optional for local endpoints   model: string;                // e.g. "qwen2.5:7b-instruct" or "qwen2.5-coder:7b-instruct"   requestTimeoutMs: number;     // default: 60000   temperature: number;          // default: 0.2   maxTokens: number;            // default: 2048   /** Prefer strict JSON response behavior, if backend supports it */   jsonMode: "strict" | "best_effort"; // default: "strict" }  export type LlmProviderConfig = LlmOpenAICompatibleConfig;  export interface SafetyConfig {   allowPatchMode: boolean;       // default: true   requireHumanReviewForPatches: boolean; // MUST be true for v0.1   patchWhitelistGlobs: string[]; // e.g. ["templates/fabric-1.20.1/src/main/java/**", ...]   denylistRegexes: string[];     // e.g. ["\\bRuntime\\.exec\\b", "\\bProcessBuilder\\b", ...]   maxPatchFiles: number;         // default: 10   maxPatchBytes: number;         // default: 200_000 }  export interface UiConfig {   kidMode: boolean;              // default: true   showDevDetails: boolean;       // default: false (can be toggled) }  export interface SettingsConfig {   schemaVersion: 1;   stt: SttCommonConfig & { providerConfig: SttProviderConfig };   llm: {     providerConfig: LlmProviderConfig;     defaultMode: LlmSafetyModeDefault;   };   safety: SafetyConfig;   ui: UiConfig; } 2) Default Settings (empfohlen) export const DEFAULT_SETTINGS: SettingsConfig = {   schemaVersion: 1,   stt: {     provider: "livekit",     language: "de-DE",     sampleRateHz: 16000,     interimResults: true,     maxUtteranceMs: 120000,     endpointingMs: 800,     providerConfig: {       provider: "livekit",       endpointUrl: "https://YOUR_STT_ENDPOINT",       apiKeyRef: "secret:livekit_api_key",       model: "default",     },   },   llm: {     providerConfig: {       provider: "openai_compatible",       baseUrl: "http://127.0.0.1:11434/v1",       apiKeyRef: "secret:llm_api_key",       model: "qwen2.5:7b-instruct",       requestTimeoutMs: 60000,       temperature: 0.2,       maxTokens: 2048,       jsonMode: "strict",     },     defaultMode: "actions_only",   },   safety: {     allowPatchMode: true,     requireHumanReviewForPatches: true,     patchWhitelistGlobs: [       "templates/fabric-1.20.1/src/main/java/**",       "templates/fabric-1.20.1/src/main/resources/**",       "generated/**",     ],     denylistRegexes: [       "\\bRuntime\\.exec\\b",       "\\bProcessBuilder\\b",       "\\bjava\\.net\\b",       "\\bsocket\\b",       "\\bFiles\\.write\\b",       "\\bFileOutputStream\\b",     ],     maxPatchFiles: 10,     maxPatchBytes: 200000,   },   ui: { kidMode: true, showDevDetails: false }, }; Faktisch korrekt sage ich, JSON Schema für SettingsConfig (Draft 2020-12) Speichere als: packages/ipc-contracts/src/schemas/settings.schema.json  {   "$schema": "https://json-schema.org/draft/2020-12/schema",   "$id": "https://kidmodstudio.local/schemas/settings.schema.json",   "title": "KidModStudio SettingsConfig",   "type": "object",   "additionalProperties": false,   "required": ["schemaVersion", "stt", "llm", "safety", "ui"],   "properties": {     "schemaVersion": { "const": 1 },      "stt": {       "type": "object",       "additionalProperties": false,       "required": [         "provider",         "language",         "sampleRateHz",         "interimResults",         "maxUtteranceMs",         "endpointingMs",         "providerConfig"       ],       "properties": {         "provider": { "enum": ["livekit", "webspeech", "manual_text"] },         "language": { "enum": ["de-DE", "en-US"] },         "sampleRateHz": { "enum": [16000, 48000] },         "interimResults": { "type": "boolean" },         "maxUtteranceMs": { "type": "integer", "minimum": 1000, "maximum": 600000 },         "endpointingMs": { "type": "integer", "minimum": 0, "maximum": 5000 },          "providerConfig": {           "oneOf": [             {               "type": "object",               "additionalProperties": false,               "required": ["provider", "endpointUrl"],               "properties": {                 "provider": { "const": "livekit" },                 "endpointUrl": { "type": "string", "minLength": 1 },                 "apiKeyRef": { "type": "string", "pattern": "^secret:.+$" },                 "model": { "type": "string" },                 "insecureSkipVerifyTLS": { "type": "boolean" }               }             },             {               "type": "object",               "additionalProperties": false,               "required": ["provider"],               "properties": { "provider": { "const": "webspeech" } }             },             {               "type": "object",               "additionalProperties": false,               "required": ["provider"],               "properties": { "provider": { "const": "manual_text" } }             }           ]         }       }     },      "llm": {       "type": "object",       "additionalProperties": false,       "required": ["providerConfig", "defaultMode"],       "properties": {         "defaultMode": { "enum": ["actions_only", "patch_fix"] },         "providerConfig": {           "type": "object",           "additionalProperties": false,           "required": [             "provider",             "baseUrl",             "model",             "requestTimeoutMs",             "temperature",             "maxTokens",             "jsonMode"           ],           "properties": {             "provider": { "const": "openai_compatible" },             "baseUrl": { "type": "string", "minLength": 1 },             "apiKeyRef": { "type": "string", "pattern": "^secret:.+$" },             "model": { "type": "string", "minLength": 1 },             "requestTimeoutMs": { "type": "integer", "minimum": 1000, "maximum": 600000 },             "temperature": { "type": "number", "minimum": 0.0, "maximum": 2.0 },             "maxTokens": { "type": "integer", "minimum": 1, "maximum": 32768 },             "jsonMode": { "enum": ["strict", "best_effort"] }           }         }       }     },      "safety": {       "type": "object",       "additionalProperties": false,       "required": [         "allowPatchMode",         "requireHumanReviewForPatches",         "patchWhitelistGlobs",         "denylistRegexes",         "maxPatchFiles",         "maxPatchBytes"       ],       "properties": {         "allowPatchMode": { "type": "boolean" },         "requireHumanReviewForPatches": { "const": true },         "patchWhitelistGlobs": {           "type": "array",           "items": { "type": "string", "minLength": 1 },           "minItems": 1         },         "denylistRegexes": {           "type": "array",           "items": { "type": "string", "minLength": 1 },           "default": []         },         "maxPatchFiles": { "type": "integer", "minimum": 1, "maximum": 100 },         "maxPatchBytes": { "type": "integer", "minimum": 1000, "maximum": 2000000 }       }     },      "ui": {       "type": "object",       "additionalProperties": false,       "required": ["kidMode", "showDevDetails"],       "properties": {         "kidMode": { "type": "boolean" },         "showDevDetails": { "type": "boolean" }       }     }   } } Faktisch korrekt sage ich, IPC Contracts (Channels + Request/Response + Events) 1) Channel Konstanten // packages/ipc-contracts/src/channels.ts export const IPC = {   // STT   sttStreamStart: "stt.streamStart",   sttStreamPush: "stt.streamPush",   sttStreamStop: "stt.streamStop",   sttStreamCancel: "stt.streamCancel",   sttStreamStatus: "stt.streamStatus",   sttStreamEvent: "stt.streamEvent",    // LLM   llmHealthCheck: "llm.healthCheck",   llmCompleteJSON: "llm.completeJSON",    // Optional Settings IO (recommended)   settingsGet: "settings.get",   settingsUpdate: "settings.update" } as const; 2) STT Streaming Types Audio Format v0.1 (fest):  PCM16LE, mono  sampleRateHz = 16000 empfohlen  Chunking: 20ms Frames (320 samples @16kHz => 640 bytes)  // packages/ipc-contracts/src/index.ts  export type SttStreamId = string;  export interface SttStreamStartReq {   streamId: SttStreamId;   settingsOverride?: Partial<SettingsConfig["stt"]>; }  export interface SttStreamStartRes {   ok: boolean;   streamId: SttStreamId;   provider: SttProviderId;   startedAtMs: number;   message?: string; }  export interface SttStreamPushReq {   streamId: SttStreamId;   chunkIndex: number;   /** PCM16LE bytes, mono */   pcm16le: Uint8Array; }  export interface SttStreamStopReq {   streamId: SttStreamId; }  export interface SttStreamStopRes {   ok: boolean;   streamId: SttStreamId;   message?: string; }  export interface SttStreamCancelReq {   streamId: SttStreamId;   reason?: string; }  export interface SttStreamStatusReq {   streamId: SttStreamId; }  export interface SttStreamStatusRes {   ok: boolean;   streamId: SttStreamId;   state: "idle" | "streaming" | "stopping" | "error";   provider: SttProviderId;   lastError?: string; }  /** Main -> Renderer event */ export type SttStreamEvent =   | {       streamId: SttStreamId;       type: "interim";       text: string;       confidence?: number;       tMs: number;     }   | {       streamId: SttStreamId;       type: "final";       text: string;       confidence?: number;       tMs: number;     }   | {       streamId: SttStreamId;       type: "state";       state: "ready" | "listening" | "processing" | "done";       tMs: number;     }   | {       streamId: SttStreamId;       type: "error";       message: string;       code?: string;       tMs: number;     }; IPC Semantik (wichtig)  stt.streamStart = ipcRenderer.invoke (Request/Response)  stt.streamPush = ipcRenderer.send (fire-and-forget; performance)  stt.streamStop = invoke  stt.streamCancel = invoke  stt.streamEvent = ipcMain → webContents.send  3) LLM Health Check export interface LlmHealthCheckReq {   settingsOverride?: Partial<SettingsConfig["llm"]>; }  export interface LlmHealthCheckRes {   ok: boolean;   baseUrl: string;   model: string;   latencyMs?: number;   message?: string; } 4) LLM completeJSON (strict contract) Intention: Renderer ruft nicht “freeform chat” auf, sondern fordert schema-konformes JSON an.  export interface LlmCompleteJSONReq {   requestId: string;    /** Optional: the app’s internal pattern id (preferred). */   patternId?: string;    /** If you don’t use patterns yet, allow explicit messages. */   messages?: Array<{ role: "system" | "user" | "assistant"; content: string }>;    /** JSON Schema that the response must satisfy. */   jsonSchema: object;    /** Variables/context the main process may inject into prompt/pattern. */   variables?: Record<string, unknown>;    /** Optional overrides */   settingsOverride?: Partial<SettingsConfig["llm"]>;    /** hard limit for response bytes to protect renderer */   maxResponseBytes?: number; }  export interface LlmCompleteJSONRes {   ok: boolean;   requestId: string;   model?: string;   latencyMs?: number;    /** Parsed JSON (already schema-validated) */   json?: unknown;    /** Raw text (only for debugging; include only if ui.showDevDetails=true) */   rawText?: string;    error?: { message: string; code?: string }; } Hard Rules (DoD relevant)  Main muss jsonSchema validieren (Ajv) gegen das Model-Output.  Bei Validation Fail: ok=false, error.code="SCHEMA_VALIDATION_FAILED", kein json.  rawText nur liefern, wenn Settings ui.showDevDetails=true (oder explizit Debug-flag).  Logisch scheint mir, die genaue Implementierungsanweisung (Dev Auftrag “end-to-end”) A) Settings Store (Main) A1. Persistenz-Ort  Datei: ${app.getPath("userData")}/settings.json  Laden bei App-Start; bei Fehlern: fallback auf DEFAULT_SETTINGS + message in logs.  A2. Runtime Validierung  Nutze Ajv im Main:  validateSettings(settings) gegen settings.schema.json  Bei invalid: backup settings.json → settings.invalid.<timestamp>.json und reset auf default.  A3. Migration  schemaVersion ist Pflicht.  migrateSettings(old): SettingsConfig:  Wenn schemaVersion fehlt: treat as v0 → default merge  Wenn unbekannt >1: reject + safe fallback  Tests: migration fixtures.  A4. Secrets  Settings speichern nur secretRef.  Implementiere SecretStore Interface (Main):  get(ref: SecretRef): Promise<string | null>  set(ref: SecretRef, value: string): Promise<void>  delete(ref: SecretRef): Promise<void>  Backend:  bevorzugt keytar (OS keychain) oder alternativ Electron safeStorage wrapper (falls keytar nicht geht).  Renderer darf Keys nie sehen; nur “isConfigured” Flags.  B) Preload Bridge (typed API) Expose im window.kidmod:  export interface KidModBridge {   stt: {     streamStart(req: SttStreamStartReq): Promise<SttStreamStartRes>;     streamPush(req: SttStreamPushReq): void;     streamStop(req: SttStreamStopReq): Promise<SttStreamStopRes>;     streamCancel(req: SttStreamCancelReq): Promise<void>;     streamStatus(req: SttStreamStatusReq): Promise<SttStreamStatusRes>;     onStreamEvent(cb: (ev: SttStreamEvent) => void): () => void;   };   llm: {     healthCheck(req: LlmHealthCheckReq): Promise<LlmHealthCheckRes>;     completeJSON(req: LlmCompleteJSONReq): Promise<LlmCompleteJSONRes>;   };   settings: {     get(): Promise<SettingsConfig>;     update(patch: Partial<SettingsConfig>): Promise<SettingsConfig>;   }; } Security requirements  Preload validiert payload grob (z.B. streamId string length; pcm16le max size).  Keine “generic invoke(channel, any)”.  C) IPC Handlers (Main) C1. stt.stream*  stt.streamStart:  lädt aktuelle settings (+ override)  erstellt streamSession in map: {streamId -> providerInstance}  sendet stt.streamEvent state ready/listening  stt.streamPush:  prüft stream exists  prüft chunk size <= z.B. 64KB  forwarded to provider  stt.streamStop:  provider finalize → send final event (falls vorhanden) → state done  cleanup session  stt.streamCancel:  abort provider → cleanup → send error/state  C2. llm.healthCheck  resolved providerConfig (settings + override)  macht minimalen request:  entweder GET /v1/models oder kurzer JSON completion (timeout 3–10s)  response: ok/model/latency/message  C3. llm.completeJSON  resolved providerConfig  compose prompt:  wenn patternId: lade pattern template + render variables  sonst: nutze messages direkt  call provider  parse result:  JSON.parse (strict) → Ajv validate with jsonSchema  return LlmCompleteJSONRes  D) Tests (Pflicht) Unit: Settings validation  valid default passes  invalid provider id fails  invalid secretRef fails pattern  requireHumanReviewForPatches must be true  Unit: IPC payload guards  reject giant pcm16le  reject missing streamId  Integration: llm.completeJSON with mocked provider  provider returns valid JSON → ok true  provider returns invalid JSON → error JSON_PARSE_FAILED  provider returns JSON but schema mismatch → SCHEMA_VALIDATION_FAILED  E) Definition of Done (für diesen Auftrag) settings.schema.json + TS types sind konsistent (keine Drift; ideal: generate schema oder snapshot test)  App startet ohne Settings Datei (erstellt default)  settings.update persistiert + validiert  IPC endpoints implementiert + typed + Preload-exposed  llm.healthCheck & llm.completeJSON funktionieren gegen Mock-Provider  STT Streaming Contract ist implementiert (Sessions + Events), Provider darf stub sein (z.B. “echo transcript”) – Hauptsache Datenpfad/Events/Errors sind korrekt